{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa313ef4",
   "metadata": {},
   "source": [
    "# News Feed Function\n",
    "Use Google News RSS to get relevant items within a selected time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import urllib.parse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from newspaper import Article\n",
    "import json\n",
    "import difflib\n",
    "import subprocess\n",
    "import requests\n",
    "\n",
    "#  Only include items on/after Sept 25, 2025\n",
    "start = datetime(2025, 9, 25, 0, 0, 0)\n",
    "\n",
    "def get_news_feed(query: str, limit: int = 15, start_date: datetime = start):\n",
    "    # Encode the query into a URL\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    feed_url = f\"https://news.google.com/rss/search?q={encoded_query}\"\n",
    "    \n",
    "    feed = feedparser.parse(feed_url)\n",
    "    length = len(feed.entries)\n",
    "\n",
    "    output = \"\"\n",
    "    output_dict = {}\n",
    "\n",
    "    for i in range(length):\n",
    "        entry = feed.entries[i]\n",
    "        published = getattr(entry, \"published\", None)\n",
    "        published_parsed = getattr(entry, \"published_parsed\", None)\n",
    "\n",
    "        # Skip if no timestamp\n",
    "        if not published_parsed:\n",
    "            continue\n",
    "\n",
    "        entry_date = datetime.fromtimestamp(time.mktime(published_parsed))\n",
    "\n",
    "        # Filter: skip if before start_date\n",
    "        if start_date and entry_date < start_date:\n",
    "            continue\n",
    "\n",
    "        title = entry.title\n",
    "        link = entry.link\n",
    "\n",
    "        output += f\"{title} - {published if published else 'No timestamp'}\\n\\n\"\n",
    "        output_dict[title] = {\n",
    "            \"link\": link,\n",
    "            \"published\": published\n",
    "        }\n",
    "\n",
    "        if len(output_dict) >= limit:\n",
    "            break\n",
    "\n",
    "    return output_dict, output.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b84aa",
   "metadata": {},
   "source": [
    "# Starter Prompt and Tools\n",
    "The main prompt which is used for all future prompts, along with the initial tools for hooking and marking items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_messages = [\n",
    "  {\"role\": \"system\", \"content\": \"\"\"\n",
    "  You are a helpful AI. You will be connected to an RSS feed based on the user's request. \n",
    "  Instead of being reactive, you will be proactive to the RSS feed and contact the user \n",
    "  when any item matches. If something might match, call the 'Mark' tool.\n",
    "   \n",
    "  There will not always be relevant items, so do not call the 'Mark' tool because you feel obligated.\n",
    "\n",
    "  Workflow:\n",
    "  1. User sends request.\n",
    "  2. Use the 'Hook' tool to create EXACTLY 7 distinct searches. YOU MAY NOT DO LESS THAN 7.\n",
    "  3. You will receive the top 5 results per search. Use 'Mark' to flag relevant ones.\n",
    "   \n",
    "  Rules for searches:\n",
    "  - No superficial variations. Do not create searches that only differ by one vague word \n",
    "    (e.g., \"OpenAI research\" vs. \"OpenAI innovation\").\n",
    "  - Each search must represent a distinct angle of the request (e.g., policy, technical \n",
    "    breakthroughs, collaborations, controversies, societal impacts).\n",
    "  - All searches must remain clearly relevant to the user request. Do not drift into \n",
    "    unrelated areas just to make them different.\n",
    "  - Keep searches concise: 2-5 words each.\n",
    "  - Prioritize recall. Err on the side of including items that might be relevant. \n",
    "    Avoid narrowing too much.\n",
    "\n",
    "  Example user request: \"I want to be notified if there's any news about governments \n",
    "  creating new regulations specifically for AI safety research.\"\n",
    "  Three good searches:\n",
    "  - \"government AI safety regulation\"\n",
    "  - \"policy frameworks for AI risk research\"\n",
    "  - \"AI governance oversight research initiatives\"\n",
    "\n",
    "  These are all relevant, but capture different aspects of the request. \n",
    "\n",
    "  Aim to reduce false negatives at all costs. If an item has ANY possibility of being relevant, you must include it. ONLY remove the titles that are OBVIOUSLY irrelevant to the user's request.\n",
    "  \"\"\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hello! I want to be notified if there's any news about climate change affecting global food security. Thanks!\"},\n",
    "]\n",
    "\n",
    "user_query = start_messages[1][\"content\"]\n",
    "\n",
    "start_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"mark\",\n",
    "            \"strict\": True,\n",
    "            \"description\": \"Mark an RSS item as relevant to the user's request.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"titles\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                          \"type\": \"string\"\n",
    "                        },\n",
    "                        \"description\": \"The titles of the relevant articles.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"titles\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "          \"name\": \"hook\",\n",
    "          \"strict\": True,\n",
    "          \"description\": \"Create an RSS feed with Google News.\",\n",
    "          \"parameters\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                  \"searches\": {\n",
    "                      \"type\": \"array\",\n",
    "                      \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                      },\n",
    "                      \"description\": \"The RSS searches you want to make. You can use spaces\"\n",
    "                  }\n",
    "              },\n",
    "              \"required\": [\"searches\"]\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19a492",
   "metadata": {},
   "source": [
    "# Chat Function\n",
    "Sends API call to Cerebras with optional tools, and repeats the call when tools are needed but not provided. If Cerebras has reached the daily limit, send it to OpenRouter. Manual switching is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "or_key = os.getenv(\"OR_KEY\")\n",
    "\n",
    "client = Cerebras(\n",
    "  api_key=api_key,\n",
    ")\n",
    "\n",
    "def cerebras_completion(messages, tools):\n",
    "  chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    model=\"llama-4-scout-17b-16e-instruct\"\n",
    "  )\n",
    "\n",
    "  message = chat_completion.choices[0].message\n",
    "  message_resp = message.content\n",
    "  tool_name, tool_contents = None, None\n",
    "\n",
    "  # Case 1: structured tool call\n",
    "  if message.tool_calls:\n",
    "      tool_name = message.tool_calls[0].function.name\n",
    "      tool_contents = message.tool_calls[0].function.arguments\n",
    "\n",
    "  # Case 2: raw JSON in content (sometimes the model forgets to use parenthesis for tools and uses brackets instead)\n",
    "  else:\n",
    "      try:\n",
    "          parsed = json.loads(message_resp)\n",
    "          if \"name\" in parsed and \"arguments\" in parsed:\n",
    "              tool_name = parsed[\"name\"]\n",
    "              tool_contents = parsed[\"arguments\"]\n",
    "          message_resp = None\n",
    "      except Exception:\n",
    "          pass\n",
    "\n",
    "  return message_resp, tool_name, tool_contents\n",
    "\n",
    "def openrouter_completion(messages, tools):\n",
    "    response = requests.post(\n",
    "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {or_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        data=json.dumps({\n",
    "            \"model\": \"meta-llama/llama-4-scout\",\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools,\n",
    "            \"provider\": {\n",
    "                \"order\": [\"cerebras\"],\n",
    "                \"allow_fallbacks\": False\n",
    "            }\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Parse top-level JSON\n",
    "    resp = response.json()\n",
    "    message = resp[\"choices\"][0][\"message\"]\n",
    "    message_resp = message.get(\"content\")\n",
    "    tool_name, tool_contents = None, None\n",
    "\n",
    "    # Case 1: structured tool call\n",
    "    if \"tool_calls\" in message and message[\"tool_calls\"]:\n",
    "        tool_name = message[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "        tool_contents = message[\"tool_calls\"][0][\"function\"][\"arguments\"]\n",
    "\n",
    "    # Case 2: raw JSON in content\n",
    "    elif message_resp:\n",
    "        try:\n",
    "            parsed = json.loads(message_resp)\n",
    "            if \"name\" in parsed and \"arguments\" in parsed:\n",
    "                tool_name = parsed[\"name\"]\n",
    "                tool_contents = parsed[\"arguments\"]\n",
    "                message_resp = None  # tool call, not plain content\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return message_resp, tool_name, tool_contents\n",
    "\n",
    "def chat(messages, tools=None, need_tool=False):\n",
    "    for _ in range(3):\n",
    "        # message, tool_name, tool_contents = cerebras_completion(messages, tools)\n",
    "\n",
    "        message, tool_name, tool_contents = openrouter_completion(messages, tools)\n",
    "\n",
    "        if need_tool and not tool_name:\n",
    "            continue # retry if a tool is required\n",
    "\n",
    "        return message, tool_name, tool_contents\n",
    "\n",
    "    # If it's still nothing\n",
    "    return message, tool_name, tool_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4d22c",
   "metadata": {},
   "source": [
    "# Filter by Title\n",
    "Get the LLM to go through all items by title and filter out obviously irrelevant ones, avoiding False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy matching since the AI sometimes does not include parts of the title\n",
    "def find_best_match(model_title, news_dict):\n",
    "    matches = difflib.get_close_matches(model_title, news_dict.keys(), n=1, cutoff=0.5)\n",
    "    if matches:\n",
    "        return news_dict[matches[0]]\n",
    "    return \"\"\n",
    "\n",
    "# Initial chat, get RSS setup\n",
    "message, tool_name, tool_contents = chat(start_messages, start_tools, True)\n",
    "searches = json.loads(tool_contents)[\"searches\"]\n",
    "\n",
    "all_rss_items = [] # everything pulled from RSS\n",
    "chosen_titles = [] # everything chosen by model\n",
    "all_news_dicts = [] # raw rss dicts\n",
    "\n",
    "print(\"=== USER PROMPT ===\")\n",
    "print(user_query)\n",
    "\n",
    "print(\"\\n=== ALL SEARCHES ===\")\n",
    "\n",
    "valid_items = 0\n",
    "\n",
    "for search in searches:\n",
    "    if valid_items >= 12:\n",
    "        print(\"! STOPPED SEARCHING !\")\n",
    "        break\n",
    "\n",
    "    output_dict, output_str = get_news_feed(search)\n",
    "\n",
    "    if output_str == '':\n",
    "        print(f\"{search} - EMPTY\")\n",
    "        continue\n",
    "\n",
    "    valid_items += len(output_dict)\n",
    "    print(search)\n",
    "\n",
    "    loop_messages = list(start_messages) + [\n",
    "        {\"role\": \"assistant\", \"content\": f\"{output_str} This is a list of the most recent RSS items for the search '{search}'. I will now use tool 'mark' if any of the items' titles seem like they could possibly apply to the user's query. I will avoid False Negatives, preferring False Positives. I will NOT use 'hook' because I already did that.\"},\n",
    "    ]\n",
    "    _, tool_name, tool_contents = chat(loop_messages, start_tools, True)\n",
    "\n",
    "    # Handle tool calling issues\n",
    "    if isinstance(tool_contents, str):\n",
    "        titles = json.loads(tool_contents)[\"titles\"]\n",
    "    elif isinstance(tool_contents, dict):\n",
    "        titles = tool_contents[\"titles\"]\n",
    "\n",
    "    all_rss_items.extend(output_dict.keys())\n",
    "    chosen_titles.extend(titles)\n",
    "    all_news_dicts.append(output_dict)\n",
    "\n",
    "# Deduplicate RSS titles\n",
    "all_rss_items = list(dict.fromkeys(all_rss_items))\n",
    "chosen_titles = list(dict.fromkeys(chosen_titles))\n",
    "\n",
    "# Merge all dicts\n",
    "combined_news_dict = {}\n",
    "for nd in all_news_dicts:\n",
    "    combined_news_dict.update(nd)\n",
    "\n",
    "# Map chosen titles to links\n",
    "chosen_dict = {}\n",
    "for t in chosen_titles:\n",
    "    chosen_dict[t] = find_best_match(t, combined_news_dict)\n",
    "\n",
    "print(\"\\n=== ALL RSS ITEMS LOOKED AT ===\")\n",
    "for item in all_rss_items:\n",
    "    print(item)\n",
    "\n",
    "print(\"\\n=== ALL CHOSEN ITEMS ===\")\n",
    "for t, meta in chosen_dict.items():\n",
    "    if isinstance(meta, dict):\n",
    "        print(f\"{t} -> {meta['link']}\")\n",
    "    else:\n",
    "        print(f\"{t} -> {meta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b81d9",
   "metadata": {},
   "source": [
    "# Filter by Content\n",
    "Get the LLM to go through the items by content, given the first 2500 chars to reduce token amount. If it passes the second filter, create ~200 word summary with important information, facts, etc. for report creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c371b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_google_news_url(url: str) -> str:\n",
    "    return subprocess.check_output(\n",
    "        [\"python\", \"resolve_google_news.py\", url],\n",
    "        text=True\n",
    "    ).strip()\n",
    "\n",
    "def get_main_content(url: str) -> str:\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        return article.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "eval_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"mark\",\n",
    "            \"strict\": True,\n",
    "            \"description\": \"Mark the article/page as relevant or not.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"relevant\": {\n",
    "                        \"type\": \"boolean\",\n",
    "                        \"description\": \"Use True if it is relevant, False if it is not.\"\n",
    "                    },\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"ALWAYS FILL THIS OUT IF RELEVANT IS TRUE. A detailed explanation (approximately 200 words). Specifically, spend most of the words on the important details and info the page contains, and the last bit on how it is relevant to the user query. For the main content, make sure to include all specific information such as names, proper nouns, places, events, times, groups, etc. Your goal is not summarization as much as it is to gather the very relevent and specific information. Do not make this generic - include the important stuff that the article goes over. Pack as much specific information in the main content as you can, and then at the very end tie it back to the user.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"relevant\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "passed_items = []\n",
    "\n",
    "for item, meta in chosen_dict.items():\n",
    "    print(f\"=== ITEM ===\")\n",
    "    print(item)\n",
    "\n",
    "    date = meta[\"published\"]\n",
    "\n",
    "    link = meta[\"link\"]\n",
    "    link = resolve_google_news_url(link)\n",
    "\n",
    "    content = get_main_content(link)[:3000]\n",
    "\n",
    "    if (len(content) < 200):\n",
    "        print(\"! Article is empty or a stub !\")\n",
    "        continue\n",
    "\n",
    "    eval_messages = list(start_messages) + [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"\n",
    "    I am currently evaluating whether this article is relevant to the user query: '{user_query}'.\n",
    "\n",
    "    ARTICLE TITLE: {item}\n",
    "    ARTICLE CONTENT (first 3000 chars):\n",
    "    {content}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. I will decide strictly if the article is relevant to the query. I will NOT mark it relevant just because it mentions a keyword. If it does not address the query, I will mark `relevant = false`.\n",
    "    2. If relevant = true, I'll:\n",
    "    - Write a detailed explanation (200-250 words).\n",
    "    - Focus on concrete details that appear in the article. I will NOT generalize.\n",
    "    - Cover at least 90% of the important content from this excerpt.\n",
    "    - End the explanation by explicitly tying the article back to the user query.\n",
    "    3. If relevant = false:\n",
    "    - I will not write any explanation or summary. I'll only return `relevant = false`.\n",
    "\n",
    "    I will not say things such as \"contains specific details\". Instead, I will provide the exact specific details, not just mention that they exist.\n",
    "    \n",
    "    Specific Details to Always Include (when present):\n",
    "    - Numbers, dates, and statistics (percentages, counts, totals, averages, ranges, rankings)\n",
    "    - Names of people and groups (individuals, organizations, companies, institutions, agencies)\n",
    "    - Geographic references (countries, cities, regions, local areas)\n",
    "    - Events and milestones (announcements, launches, agreements, disasters, protests, meetings)\n",
    "    - Quotes and statements (from officials, experts, witnesses, participants)\n",
    "    - Policies and rules (laws, regulations, programs, reforms, restrictions, standards)\n",
    "    - Technologies and methods (tools, systems, processes, techniques)\n",
    "    - Economic indicators (prices, costs, investments, budgets, trade figures)\n",
    "    - Social impacts (effects on communities, health, education, migration, lifestyles)\n",
    "    - Environmental factors (weather, climate, land, water, resources, ecosystems)\n",
    "    - Other obviously relevant things not on this list.\n",
    "\n",
    "    I am REQUIRED to say ALL specific details I see that are relevant. I will NOT cut ANY of them.\n",
    "    \n",
    "    Additionally, I will use quotes for important information that matters verbatum.\n",
    "    I will *literally* use a minimum of 200 words.\n",
    "\n",
    "    My output must strictly use the `mark` function schema.\n",
    "    \"\"\"\n",
    "        }\n",
    "    ]\n",
    "    _, tool_name, tool_contents = chat(eval_messages, eval_tools, True)\n",
    "\n",
    "    # Handle tool calling issues\n",
    "    parsed = None\n",
    "    if tool_contents:\n",
    "        if isinstance(tool_contents, dict):\n",
    "            parsed = tool_contents\n",
    "        else:\n",
    "            try:\n",
    "                parsed = json.loads(tool_contents)\n",
    "            except json.JSONDecodeError:\n",
    "                fixed = tool_contents.replace(\"true\", \"True\").replace(\"false\", \"False\")\n",
    "                try:\n",
    "                    parsed = eval(fixed, {\"__builtins__\": None}, {})\n",
    "                except Exception:\n",
    "                    parsed = tool_contents\n",
    "\n",
    "    # If the AI marked the item as relevant, add to list\n",
    "    if parsed and isinstance(parsed, dict) and parsed.get(\"relevant\") == True:\n",
    "        passed_items.append([item, link, date, parsed.get(\"reason\", \"\")])\n",
    "\n",
    "    print(tool_name, parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177e24b",
   "metadata": {},
   "source": [
    "# Report Generation\n",
    "Generate the report given the list of items. Report is made in Markdown, using links for all sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70159da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_str(items):\n",
    "    full = \"\"\n",
    "    for name, link, date, reason in items:\n",
    "        full += f\"=== ITEM NAME ===\\n{name}\\n\"\n",
    "        full += f\"=== ITEM LINK (To cite) ===\\n{link}\\n\"\n",
    "        full += f\"=== ITEM DATE ===\\n{date}\\n\"\n",
    "        full += f\"=== ITEM INFO (LLM generated) ===\\n{reason}\\n\\n\"\n",
    "    return full\n",
    "\n",
    "report_messages = list(start_messages) + [\n",
    "    {\"role\": \"assistant\", \"content\": f\"\"\"\n",
    "    {create_content_str(passed_items)}\n",
    "    These are all items relevant to the query: '{user_query}'.\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Write 750 words AT MINIMUM in Markdown with clear structure using # (H1) and ## (H2) headings. Do NOT overuse these - you can and SHOULD do multiple paragraphs under one.\n",
    "    2. Use the most reputable source for each piece of information and avoid duplication.\n",
    "    3. Use specific information such as numbers, events, people, etc. where useful; do not avoid using these.\n",
    "    4. Naturally connect all information back to the query, and combine sources when appropriate.\n",
    "    5. Begin by addressing the query directly, explaining what has developed since the last interaction ({start}) up to today ({datetime.now()}), including how much time has passed.\n",
    "    6. Never write dates (like \"2025-09-29\", \"Sep 29, 2025\", or UTC strings). Always write relative time only, e.g. \"3 hours ago\", \"2 days ago\", or \"2 weeks ago\".\n",
    "    7. Conclude by explaining why the updates matter, adding context rather than summarizing obvious knowledge.\n",
    "    8. Do not mention being an AI or proactive agent, and do not use words like \"proactive.\" Write directly to the reader (\"you\") when appropriate.\n",
    "    9. Always cite inline like this: ([Source Website Name](https://example.com) - TIME AGO). \n",
    "       - Parentheses must wrap the citation. \n",
    "       - The clickable text must ALWAYS be the EXACT website name, NOT the article title, NOT the raw link.\n",
    "       - Place citations immediately after the information, not at the end.\n",
    "       - YOU ARE NOT ALLOWED TO CASUALLY CITE THINGS LIKE \"For example, SOURCE said...\" YOU ARE REQUIRED TO CITE IT AT THE END OF TALKING ABOUT THE CONTENT.\n",
    "    \n",
    "    CONTEXT:\n",
    "    The goal is to provide timely updates on new developments since the last interaction, not background knowledge. The writing should feel polished, informative, and up-to-date.\n",
    "\n",
    "    Remember, 750 words MINIMUM.\n",
    "    \"\"\"}\n",
    "]\n",
    "message, _, _ = chat(report_messages)\n",
    "\n",
    "print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
